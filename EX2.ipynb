{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EX2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Hamahmi/CIFAR-10/blob/master/EX2.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Io3xMahdr_O1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "442b6528-78e4-413a-85ac-bda53cc09336"
      },
      "cell_type": "code",
      "source": [
        "# ========================================================== #\n",
        "# File name: retrain_cifar-10_bn.py\n",
        "# Author: BIGBALLON\n",
        "# Date created: 07/27/2017\n",
        "# Python Version: 3.5.2\n",
        "# Tensorflow Vetsion: 1.2.1\n",
        "# Result: test accuracy about 93.50 ~ 93.70%\n",
        "# ========================================================== #\n",
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D\n",
        "from keras.initializers import he_normal\n",
        "from keras import optimizers\n",
        "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils.data_utils import get_file\n",
        "\n",
        "num_classes  = 10\n",
        "batch_size   = 128\n",
        "epochs       = 200\n",
        "iterations   = 391\n",
        "dropout      = 0.5\n",
        "weight_decay = 0.0001\n",
        "log_filepath = r'./vgg19_retrain_logs/'\n",
        "\n",
        "from keras import backend as K\n",
        "if('tensorflow' == K.backend()):\n",
        "    import tensorflow as tf\n",
        "    from keras.backend.tensorflow_backend import set_session\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    sess = tf.Session(config=config)\n",
        "\n",
        "def scheduler(epoch):\n",
        "    if epoch < 80:\n",
        "        return 0.1\n",
        "    if epoch < 160:\n",
        "        return 0.01\n",
        "    return 0.001\n",
        "\n",
        "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "filepath = get_file('vgg19_weights_tf_dim_ordering_tf_kernels.h5', WEIGHTS_PATH, cache_subdir='models')\n",
        "\n",
        "# data loading\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# data preprocessing \n",
        "x_train[:,:,:,0] = (x_train[:,:,:,0]-123.680)\n",
        "x_train[:,:,:,1] = (x_train[:,:,:,1]-116.779)\n",
        "x_train[:,:,:,2] = (x_train[:,:,:,2]-103.939)\n",
        "x_test[:,:,:,0] = (x_test[:,:,:,0]-123.680)\n",
        "x_test[:,:,:,1] = (x_test[:,:,:,1]-116.779)\n",
        "x_test[:,:,:,2] = (x_test[:,:,:,2]-103.939)\n",
        "\n",
        "# build model\n",
        "model = Sequential()\n",
        "\n",
        "# Block 1\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block1_conv1', input_shape=x_train.shape[1:]))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block1_conv2'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'))\n",
        "\n",
        "# Block 2\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block2_conv1'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block2_conv2'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'))\n",
        "\n",
        "# Block 3\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block3_conv1'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block3_conv2'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block3_conv3'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block3_conv4'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool'))\n",
        "\n",
        "# Block 4\n",
        "model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block4_conv1'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block4_conv2'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block4_conv3'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block4_conv4'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool'))\n",
        "\n",
        "# Block 5\n",
        "model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block5_conv1'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block5_conv2'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block5_conv3'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block5_conv4'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool'))\n",
        "\n",
        "# model modification for cifar-10\n",
        "model.add(Flatten(name='flatten'))\n",
        "model.add(Dense(4096, use_bias = True, kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='fc_cifa10'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Dense(4096, kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='fc2'))  \n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dropout))      \n",
        "model.add(Dense(10, kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='predictions_cifa10'))        \n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# load pretrained weight from VGG19 by name      \n",
        "model.load_weights(filepath, by_name=True)\n",
        "\n",
        "# -------- optimizer setting -------- #\n",
        "sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
        "change_lr = LearningRateScheduler(scheduler)\n",
        "cbks = [change_lr,tb_cb]\n",
        "\n",
        "print(\"Model\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GYWp-ejUsS3y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5543
        },
        "outputId": "17de30a1-d168-4ea6-a025-381015f56ab2"
      },
      "cell_type": "code",
      "source": [
        "print('Using real-time data augmentation.')\n",
        "datagen = ImageDataGenerator(horizontal_flip=True,\n",
        "        width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                    steps_per_epoch=iterations,\n",
        "                    epochs=epochs,\n",
        "                    callbacks=cbks,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "print(\"trained\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "Epoch 1/200\n",
            "391/391 [==============================] - 108s 276ms/step - loss: 1.8820 - acc: 0.6544 - val_loss: 2.0356 - val_acc: 0.6334\n",
            "Epoch 2/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 1.5209 - acc: 0.7558 - val_loss: 1.7360 - val_acc: 0.6829\n",
            "Epoch 3/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 1.3519 - acc: 0.7924 - val_loss: 1.5256 - val_acc: 0.7189\n",
            "Epoch 4/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 1.2253 - acc: 0.8134 - val_loss: 1.2545 - val_acc: 0.7947\n",
            "Epoch 5/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 1.1269 - acc: 0.8286 - val_loss: 1.3348 - val_acc: 0.7599\n",
            "Epoch 6/200\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 1.0471 - acc: 0.8438 - val_loss: 1.2429 - val_acc: 0.7793\n",
            "Epoch 7/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.9822 - acc: 0.8507 - val_loss: 1.2244 - val_acc: 0.7710\n",
            "Epoch 8/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.9283 - acc: 0.8582 - val_loss: 1.3419 - val_acc: 0.7411\n",
            "Epoch 9/200\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 0.8822 - acc: 0.8652 - val_loss: 1.1802 - val_acc: 0.7777\n",
            "Epoch 10/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.8395 - acc: 0.8732 - val_loss: 0.9779 - val_acc: 0.8230\n",
            "Epoch 11/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.8040 - acc: 0.8770 - val_loss: 1.0368 - val_acc: 0.8058\n",
            "Epoch 12/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.7814 - acc: 0.8807 - val_loss: 0.9417 - val_acc: 0.8276\n",
            "Epoch 13/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.7497 - acc: 0.8865 - val_loss: 0.8791 - val_acc: 0.8472\n",
            "Epoch 14/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.7365 - acc: 0.8889 - val_loss: 1.0926 - val_acc: 0.7837\n",
            "Epoch 15/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.7225 - acc: 0.8913 - val_loss: 0.9259 - val_acc: 0.8283\n",
            "Epoch 16/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.7062 - acc: 0.8939 - val_loss: 0.8742 - val_acc: 0.8404\n",
            "Epoch 17/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6924 - acc: 0.8977 - val_loss: 1.0086 - val_acc: 0.8023\n",
            "Epoch 18/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6815 - acc: 0.8977 - val_loss: 0.9753 - val_acc: 0.8064\n",
            "Epoch 19/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6784 - acc: 0.8983 - val_loss: 0.9058 - val_acc: 0.8249\n",
            "Epoch 20/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6700 - acc: 0.9019 - val_loss: 0.8445 - val_acc: 0.8443\n",
            "Epoch 21/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6629 - acc: 0.9057 - val_loss: 0.8922 - val_acc: 0.8406\n",
            "Epoch 22/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6549 - acc: 0.9073 - val_loss: 1.0157 - val_acc: 0.8027\n",
            "Epoch 23/200\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 0.6568 - acc: 0.9075 - val_loss: 0.9619 - val_acc: 0.8212\n",
            "Epoch 24/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6542 - acc: 0.9088 - val_loss: 1.0367 - val_acc: 0.8008\n",
            "Epoch 25/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6479 - acc: 0.9098 - val_loss: 0.8998 - val_acc: 0.8270\n",
            "Epoch 26/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6495 - acc: 0.9105 - val_loss: 0.9476 - val_acc: 0.8254\n",
            "Epoch 27/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6463 - acc: 0.9130 - val_loss: 1.0668 - val_acc: 0.7973\n",
            "Epoch 28/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6466 - acc: 0.9139 - val_loss: 0.9973 - val_acc: 0.8203\n",
            "Epoch 29/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6440 - acc: 0.9148 - val_loss: 1.0876 - val_acc: 0.7901\n",
            "Epoch 30/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6442 - acc: 0.9159 - val_loss: 0.9228 - val_acc: 0.8369\n",
            "Epoch 31/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6435 - acc: 0.9185 - val_loss: 0.8558 - val_acc: 0.8562\n",
            "Epoch 32/200\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 0.6404 - acc: 0.9191 - val_loss: 0.8662 - val_acc: 0.8544\n",
            "Epoch 33/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6431 - acc: 0.9190 - val_loss: 0.8874 - val_acc: 0.8500\n",
            "Epoch 34/200\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 0.6461 - acc: 0.9191 - val_loss: 0.9006 - val_acc: 0.8477\n",
            "Epoch 35/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6459 - acc: 0.9194 - val_loss: 0.9169 - val_acc: 0.8421\n",
            "Epoch 36/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6399 - acc: 0.9214 - val_loss: 1.2068 - val_acc: 0.7897\n",
            "Epoch 37/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6442 - acc: 0.9215 - val_loss: 1.1386 - val_acc: 0.7939\n",
            "Epoch 38/200\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 0.6446 - acc: 0.9206 - val_loss: 0.9356 - val_acc: 0.8443\n",
            "Epoch 39/200\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 0.6426 - acc: 0.9240 - val_loss: 0.8403 - val_acc: 0.8697\n",
            "Epoch 40/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6472 - acc: 0.9226 - val_loss: 1.1342 - val_acc: 0.7897\n",
            "Epoch 41/200\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 0.6409 - acc: 0.9246 - val_loss: 0.8802 - val_acc: 0.8601\n",
            "Epoch 42/200\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 0.6450 - acc: 0.9245 - val_loss: 1.0802 - val_acc: 0.8021\n",
            "Epoch 43/200\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 0.6462 - acc: 0.9252 - val_loss: 0.8603 - val_acc: 0.8601\n",
            "Epoch 44/200\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 0.6524 - acc: 0.9241 - val_loss: 0.9446 - val_acc: 0.8402\n",
            "Epoch 45/200\n",
            "391/391 [==============================] - 101s 260ms/step - loss: 0.6441 - acc: 0.9277 - val_loss: 1.0402 - val_acc: 0.8147\n",
            "Epoch 46/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6503 - acc: 0.9256 - val_loss: 0.8963 - val_acc: 0.8521\n",
            "Epoch 47/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6455 - acc: 0.9276 - val_loss: 0.8862 - val_acc: 0.8623\n",
            "Epoch 48/200\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 0.6554 - acc: 0.9257 - val_loss: 1.2963 - val_acc: 0.7635\n",
            "Epoch 49/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6588 - acc: 0.9263 - val_loss: 0.9217 - val_acc: 0.8499\n",
            "Epoch 50/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6462 - acc: 0.9294 - val_loss: 0.9917 - val_acc: 0.8316\n",
            "Epoch 51/200\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 0.6566 - acc: 0.9272 - val_loss: 0.9780 - val_acc: 0.8394\n",
            "Epoch 52/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6496 - acc: 0.9292 - val_loss: 1.0840 - val_acc: 0.8177\n",
            "Epoch 53/200\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 0.6537 - acc: 0.9287 - val_loss: 0.9406 - val_acc: 0.8475\n",
            "Epoch 54/200\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 0.6575 - acc: 0.9270 - val_loss: 0.9003 - val_acc: 0.8547\n",
            "Epoch 55/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6492 - acc: 0.9307 - val_loss: 1.0049 - val_acc: 0.8312\n",
            "Epoch 56/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6521 - acc: 0.9302 - val_loss: 1.0059 - val_acc: 0.8437\n",
            "Epoch 57/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6510 - acc: 0.9314 - val_loss: 1.0819 - val_acc: 0.8401\n",
            "Epoch 58/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6585 - acc: 0.9284 - val_loss: 1.0366 - val_acc: 0.8290\n",
            "Epoch 59/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6522 - acc: 0.9328 - val_loss: 0.9629 - val_acc: 0.8437\n",
            "Epoch 60/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6649 - acc: 0.9284 - val_loss: 0.9167 - val_acc: 0.8573\n",
            "Epoch 61/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6558 - acc: 0.9324 - val_loss: 1.0061 - val_acc: 0.8363\n",
            "Epoch 62/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6561 - acc: 0.9310 - val_loss: 0.9117 - val_acc: 0.8510\n",
            "Epoch 63/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6518 - acc: 0.9332 - val_loss: 1.1864 - val_acc: 0.7946\n",
            "Epoch 64/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6597 - acc: 0.9303 - val_loss: 1.0056 - val_acc: 0.8306\n",
            "Epoch 65/200\n",
            "391/391 [==============================] - 103s 263ms/step - loss: 0.6605 - acc: 0.9317 - val_loss: 0.9546 - val_acc: 0.8439\n",
            "Epoch 66/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6608 - acc: 0.9310 - val_loss: 1.0280 - val_acc: 0.8327\n",
            "Epoch 67/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6611 - acc: 0.9316 - val_loss: 1.0465 - val_acc: 0.8275\n",
            "Epoch 68/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6633 - acc: 0.9323 - val_loss: 0.8821 - val_acc: 0.8657\n",
            "Epoch 69/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6648 - acc: 0.9324 - val_loss: 1.0483 - val_acc: 0.8274\n",
            "Epoch 70/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6619 - acc: 0.9316 - val_loss: 1.0425 - val_acc: 0.8219\n",
            "Epoch 71/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6620 - acc: 0.9336 - val_loss: 1.2633 - val_acc: 0.7885\n",
            "Epoch 72/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6680 - acc: 0.9308 - val_loss: 0.9241 - val_acc: 0.8536\n",
            "Epoch 73/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6596 - acc: 0.9341 - val_loss: 1.0534 - val_acc: 0.8271\n",
            "Epoch 74/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6603 - acc: 0.9338 - val_loss: 0.9709 - val_acc: 0.8479\n",
            "Epoch 75/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6622 - acc: 0.9330 - val_loss: 0.8429 - val_acc: 0.8806\n",
            "Epoch 76/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6634 - acc: 0.9332 - val_loss: 0.8893 - val_acc: 0.8677\n",
            "Epoch 77/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6570 - acc: 0.9361 - val_loss: 0.9604 - val_acc: 0.8517\n",
            "Epoch 78/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6638 - acc: 0.9344 - val_loss: 0.9771 - val_acc: 0.8484\n",
            "Epoch 79/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6639 - acc: 0.9341 - val_loss: 1.0105 - val_acc: 0.8384\n",
            "Epoch 80/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.6683 - acc: 0.9337 - val_loss: 1.0087 - val_acc: 0.8365\n",
            "Epoch 81/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.5807 - acc: 0.9615 - val_loss: 0.7310 - val_acc: 0.9149\n",
            "Epoch 82/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.5243 - acc: 0.9773 - val_loss: 0.7135 - val_acc: 0.9211\n",
            "Epoch 83/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.5021 - acc: 0.9829 - val_loss: 0.7053 - val_acc: 0.9238\n",
            "Epoch 84/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.4874 - acc: 0.9854 - val_loss: 0.7016 - val_acc: 0.9265\n",
            "Epoch 85/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.4730 - acc: 0.9878 - val_loss: 0.6987 - val_acc: 0.9248\n",
            "Epoch 86/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.4596 - acc: 0.9904 - val_loss: 0.6961 - val_acc: 0.9264\n",
            "Epoch 87/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.4529 - acc: 0.9901 - val_loss: 0.7010 - val_acc: 0.9260\n",
            "Epoch 88/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.4406 - acc: 0.9920 - val_loss: 0.6915 - val_acc: 0.9278\n",
            "Epoch 89/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.4333 - acc: 0.9923 - val_loss: 0.6879 - val_acc: 0.9267\n",
            "Epoch 90/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.4261 - acc: 0.9924 - val_loss: 0.6808 - val_acc: 0.9280\n",
            "Epoch 91/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.4186 - acc: 0.9933 - val_loss: 0.6871 - val_acc: 0.9268\n",
            "Epoch 92/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.4105 - acc: 0.9941 - val_loss: 0.6679 - val_acc: 0.9281\n",
            "Epoch 93/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.4026 - acc: 0.9947 - val_loss: 0.6810 - val_acc: 0.9268\n",
            "Epoch 94/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.3951 - acc: 0.9954 - val_loss: 0.6739 - val_acc: 0.9275\n",
            "Epoch 95/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.3885 - acc: 0.9956 - val_loss: 0.6694 - val_acc: 0.9292\n",
            "Epoch 96/200\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 0.3829 - acc: 0.9956 - val_loss: 0.6752 - val_acc: 0.9269\n",
            "Epoch 97/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.3758 - acc: 0.9960 - val_loss: 0.6593 - val_acc: 0.9306\n",
            "Epoch 98/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.3694 - acc: 0.9965 - val_loss: 0.6608 - val_acc: 0.9293\n",
            "Epoch 99/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.3630 - acc: 0.9967 - val_loss: 0.6672 - val_acc: 0.9283\n",
            "Epoch 100/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.3599 - acc: 0.9961 - val_loss: 0.6488 - val_acc: 0.9305\n",
            "Epoch 101/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.3537 - acc: 0.9965 - val_loss: 0.6474 - val_acc: 0.9305\n",
            "Epoch 102/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.3461 - acc: 0.9973 - val_loss: 0.6532 - val_acc: 0.9286\n",
            "Epoch 103/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.3430 - acc: 0.9964 - val_loss: 0.6508 - val_acc: 0.9269\n",
            "Epoch 104/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.3375 - acc: 0.9966 - val_loss: 0.6311 - val_acc: 0.9317\n",
            "Epoch 105/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.3332 - acc: 0.9966 - val_loss: 0.6428 - val_acc: 0.9291\n",
            "Epoch 106/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.3277 - acc: 0.9969 - val_loss: 0.6318 - val_acc: 0.9281\n",
            "Epoch 107/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.3213 - acc: 0.9975 - val_loss: 0.6468 - val_acc: 0.9238\n",
            "Epoch 108/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.3189 - acc: 0.9966 - val_loss: 0.6260 - val_acc: 0.9271\n",
            "Epoch 109/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.3143 - acc: 0.9969 - val_loss: 0.6237 - val_acc: 0.9289\n",
            "Epoch 110/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.3080 - acc: 0.9973 - val_loss: 0.6221 - val_acc: 0.9274\n",
            "Epoch 111/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.3038 - acc: 0.9971 - val_loss: 0.6216 - val_acc: 0.9297\n",
            "Epoch 112/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.3008 - acc: 0.9970 - val_loss: 0.6266 - val_acc: 0.9274\n",
            "Epoch 113/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2941 - acc: 0.9975 - val_loss: 0.6173 - val_acc: 0.9289\n",
            "Epoch 114/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2894 - acc: 0.9976 - val_loss: 0.6242 - val_acc: 0.9286\n",
            "Epoch 115/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2864 - acc: 0.9974 - val_loss: 0.6036 - val_acc: 0.9302\n",
            "Epoch 116/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2816 - acc: 0.9978 - val_loss: 0.6139 - val_acc: 0.9278\n",
            "Epoch 117/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2797 - acc: 0.9971 - val_loss: 0.6066 - val_acc: 0.9282\n",
            "Epoch 118/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2755 - acc: 0.9972 - val_loss: 0.6050 - val_acc: 0.9273\n",
            "Epoch 119/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2719 - acc: 0.9972 - val_loss: 0.5936 - val_acc: 0.9289\n",
            "Epoch 120/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2688 - acc: 0.9969 - val_loss: 0.5982 - val_acc: 0.9278\n",
            "Epoch 121/200\n",
            "391/391 [==============================] - 100s 254ms/step - loss: 0.2637 - acc: 0.9974 - val_loss: 0.5819 - val_acc: 0.9264\n",
            "Epoch 122/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2590 - acc: 0.9979 - val_loss: 0.5949 - val_acc: 0.9278\n",
            "Epoch 123/200\n",
            "391/391 [==============================] - 100s 255ms/step - loss: 0.2546 - acc: 0.9982 - val_loss: 0.5944 - val_acc: 0.9273\n",
            "Epoch 124/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2534 - acc: 0.9973 - val_loss: 0.5957 - val_acc: 0.9259\n",
            "Epoch 125/200\n",
            "391/391 [==============================] - 100s 255ms/step - loss: 0.2500 - acc: 0.9973 - val_loss: 0.5885 - val_acc: 0.9276\n",
            "Epoch 126/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2471 - acc: 0.9972 - val_loss: 0.5863 - val_acc: 0.9271\n",
            "Epoch 127/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2418 - acc: 0.9979 - val_loss: 0.5840 - val_acc: 0.9240\n",
            "Epoch 128/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2418 - acc: 0.9969 - val_loss: 0.5914 - val_acc: 0.9264\n",
            "Epoch 129/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2363 - acc: 0.9977 - val_loss: 0.5737 - val_acc: 0.9281\n",
            "Epoch 130/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2332 - acc: 0.9973 - val_loss: 0.5722 - val_acc: 0.9246\n",
            "Epoch 131/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2319 - acc: 0.9968 - val_loss: 0.5793 - val_acc: 0.9250\n",
            "Epoch 132/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2280 - acc: 0.9971 - val_loss: 0.6059 - val_acc: 0.9204\n",
            "Epoch 133/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2241 - acc: 0.9976 - val_loss: 0.5634 - val_acc: 0.9274\n",
            "Epoch 134/200\n",
            "391/391 [==============================] - 100s 254ms/step - loss: 0.2219 - acc: 0.9971 - val_loss: 0.5633 - val_acc: 0.9287\n",
            "Epoch 135/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2210 - acc: 0.9966 - val_loss: 0.5565 - val_acc: 0.9236\n",
            "Epoch 136/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2177 - acc: 0.9969 - val_loss: 0.5573 - val_acc: 0.9244\n",
            "Epoch 137/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2161 - acc: 0.9964 - val_loss: 0.5388 - val_acc: 0.9266\n",
            "Epoch 138/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2113 - acc: 0.9968 - val_loss: 0.5491 - val_acc: 0.9235\n",
            "Epoch 139/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2096 - acc: 0.9971 - val_loss: 0.5367 - val_acc: 0.9254\n",
            "Epoch 140/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2057 - acc: 0.9972 - val_loss: 0.5433 - val_acc: 0.9239\n",
            "Epoch 141/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.2037 - acc: 0.9972 - val_loss: 0.5495 - val_acc: 0.9244\n",
            "Epoch 142/200\n",
            "391/391 [==============================] - 100s 255ms/step - loss: 0.2008 - acc: 0.9974 - val_loss: 0.5423 - val_acc: 0.9294\n",
            "Epoch 143/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.1995 - acc: 0.9966 - val_loss: 0.5603 - val_acc: 0.9209\n",
            "Epoch 144/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.1988 - acc: 0.9964 - val_loss: 0.5647 - val_acc: 0.9237\n",
            "Epoch 145/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.1956 - acc: 0.9969 - val_loss: 0.5272 - val_acc: 0.9261\n",
            "Epoch 146/200\n",
            "391/391 [==============================] - 104s 267ms/step - loss: 0.1933 - acc: 0.9966 - val_loss: 0.5453 - val_acc: 0.9198\n",
            "Epoch 147/200\n",
            "391/391 [==============================] - 105s 268ms/step - loss: 0.1902 - acc: 0.9969 - val_loss: 0.5249 - val_acc: 0.9248\n",
            "Epoch 148/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.1906 - acc: 0.9960 - val_loss: 0.5251 - val_acc: 0.9223\n",
            "Epoch 149/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.1856 - acc: 0.9970 - val_loss: 0.5260 - val_acc: 0.9229\n",
            "Epoch 150/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.1856 - acc: 0.9962 - val_loss: 0.5349 - val_acc: 0.9210\n",
            "Epoch 151/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.1839 - acc: 0.9964 - val_loss: 0.5052 - val_acc: 0.9265\n",
            "Epoch 152/200\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.1799 - acc: 0.9971 - val_loss: 0.5030 - val_acc: 0.9252\n",
            "Epoch 153/200\n",
            " 10/391 [..............................] - ETA: 1:31 - loss: 0.1739 - acc: 0.9992Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P2QUQQkOsYXn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "97a9bc22-b053-4d36-da10-c5be6da4982e"
      },
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 12s 1ms/step\n",
            "Test loss: 0.494263947057724\n",
            "Test accuracy: 0.9322\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}